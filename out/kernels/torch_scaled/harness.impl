#define ONEAPI_BACKEND_LEVEL_ZERO_EXT
#define DPCT_PROFILING_ENABLED
#include <sycl/sycl.hpp>
#include <dpct/dpct.hpp>
#include <iostream>
#include <fstream>
#include <random>
#include <cmath>

#include <omp.h>

void write_matrix_to_csv(const std::string& filename, float* matrix, int rows, int cols) {
    std::ofstream file(filename);
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < cols; j++) {
            file << matrix[i * cols + j];
            if (j < cols - 1) {
                file << ",";
            }
        }
        file << "\n";
    }
    file.close();
}

void cpu_gemm(float* a, float* b, float* c, int M, int N, int K) {
    std::cout << "CPU M=" << M << " N=" << N << " K=" << K << std::endl;
    #pragma omp parallel for collapse(2) // otherwise the CPU version takes for everrrrrr
    for (int i = 0; i < M; i++) {
        for (int j = 0; j < N; j++) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += a[i * K + k] * b[j * K + k]; // mma_ABt
            }
            c[i * N + j] = sum;
        }
    }
}


template<typename mmt>
int run_benchmark(size_t M, size_t N, size_t K) {
    dpct::err0 cudaStatus;

    std::cout << "--------------------  M=" << M << " N=" << N << " K=" << K << "  --------------------\n";

    // Allocate host memory
    float *h_A = new float[M * K];
    float *h_B = new float[K * N];
    float *h_C = new float[M * N];
    float *h_C_ref = new float[M * N];

    std::cout << "Allocated host memory" << std::endl;

    // Initialize random number generator
    std::random_device rd;
    std::mt19937 gen(42);
    std::normal_distribution dis(0.0f, 1.0f);

    // Initialize matrices with random values
    // for (int i = 0; i < M * K; ++i) h_A[i] = i / 100000.0f;  // dis(gen) * 0.2f; 
    // for (int i = 0; i < K * N; ++i) h_B[i] = i / 100000.0f;   // dis(gen) * 0.2f; 
    for (int i = 0; i < M * K; ++i) h_A[i] = dis(gen) * 0.2f; 
    for (int i = 0; i < K * N; ++i) h_B[i] = dis(gen) * 0.2f; 

    std::cout << "Initialized matrices" << std::endl;

    // Allocate device memory
    fp8e4m3 *d_A, *d_B;
    c_dtype *d_C;
    cudaMalloc(&d_A, M*K*sizeof(fp8e4m3));
    cudaMalloc(&d_B, K*N*sizeof(fp8e4m3));
    cudaMalloc(&d_C, M*N*sizeof(c_dtype));
    // scales
    c_dtype *d_scale_a, *d_scale_b;
    cudaMalloc(&d_scale_a, M*sizeof(c_dtype));
    cudaMalloc(&d_scale_b, N*sizeof(c_dtype));

    // Check for CUDA errors
    /*
    DPCT1010:592: SYCL uses exceptions to report errors and does not use the
    error codes. The cudaGetLastError function call was replaced with 0. You
    need to rewrite this code.
    */
    cudaStatus = 0;
    /*
    DPCT1000:589: Error handling if-stmt was detected but could not be
    rewritten.
    */
    if (cudaStatus != 0) {
        /*
        DPCT1009:593: SYCL reports errors using exceptions and does not use
        error codes. Please replace the "get_error_string_dummy(...)" with a
        real error-handling function.
        */
        /*
        DPCT1001:588: The statement could not be removed.
        */
        std::cerr << "CUDA error: " << dpct::get_error_string_dummy(cudaStatus)
                  << std::endl;
        // Optionally, you might want to exit the program or handle the error in some way
        return -1;
    }

    std::cout << "Allocated device memory" << std::endl;

    // Perform CPU matrix multiplication for reference
    if(true) cpu_gemm(h_A, h_B, h_C_ref, M, N, K);
    std::cout << "Performed CPU matrix multiplication" << std::endl;

    //  Obtain inputs on GPU device
    const float FP8_E4M3_MAX = 448.0f;
    // const float FP8_E4M3_MIN = -448.0f;
    c_dtype *h_scale_a = new c_dtype[M];
    c_dtype *h_scale_b = new c_dtype[N];
    __nv_fp8_e4m3 *h_A_fp8_scaled = new __nv_fp8_e4m3[M * K];
    __nv_fp8_e4m3 *h_B_fp8_scaled = new __nv_fp8_e4m3[K * N];
    
    // row-wise scaling
    for(int row = 0; row < M; row++) {
        float max_val = 0.0f;
        for(int col = 0; col < K; col++) {
            float abs_val = std::abs(h_A[row * K + col]);
            max_val = std::max(max_val, abs_val);
        }
        h_scale_a[row] = c_dtype(max_val / FP8_E4M3_MAX); 
        if ( row < 10 ) {
            std::cout << "h_scale_a[" << row << "] = " << float(h_scale_a[row]) << ", max_val: " << max_val << std::endl;
        }
    }

    // fill h_A_fp8_scaled by following to_float8_e4m3fn. 
    for(int i = 0; i < M; i++) {
        for(int j = 0; j < K; j++) {
            h_A_fp8_scaled[i * K + j] = __nv_fp8_e4m3(h_A[i * K + j] / float(h_scale_a[i]));
        }
    }

    // column-wise scaling
    for(int col = 0; col < N; col++) {
        float max_val = 0.0f;
        for(int row = 0; row < K; row++) {
            float abs_val = std::abs(h_B[row + col*K]);
            max_val = std::max(max_val, abs_val);
        }
        h_scale_b[col] = c_dtype(max_val / FP8_E4M3_MAX);

        if ( col < 10 ) {
            std::cout << "h_scale_b[" << col << "] = " << float(h_scale_b[col]) << ", max_val: " << max_val << std::endl;
        }
    }

    // fill h_B_fp8_scaled by following to_float8_e4m3fn
    for(int i = 0; i < N; i++) {
        for(int j = 0; j < K; j++) {
            h_B_fp8_scaled[j + i * K] = __nv_fp8_e4m3(h_B[j + i * K] / float(h_scale_b[i]));
        }
    }

    dpct::get_in_order_queue()
        .memcpy(d_A, h_A_fp8_scaled, M * K * sizeof(fp8e4m3))
        .wait();
    dpct::get_in_order_queue()
        .memcpy(d_B, h_B_fp8_scaled, K * N * sizeof(fp8e4m3))
        .wait();
    dpct::get_in_order_queue()
        .memcpy(d_scale_a, h_scale_a, M * sizeof(c_dtype))
        .wait();
    dpct::get_in_order_queue()
        .memcpy(d_scale_b, h_scale_b, N * sizeof(c_dtype))
        .wait();

    /* 
    Launch kernel
    */
    std::cout << "Copied matrices to device" << std::endl;
    unsigned long mem_size = MAX_SHARED_MEMORY - 1024;
    cudaFuncSetAttribute(prototype::lcf::kernel<mmt>, cudaFuncAttributeMaxDynamicSharedMemorySize, mem_size);

    // Launch kernel
    dpct::dim3 grid(mmt::grid(M, N, K));
    dpct::dim3 block(kittens::prototype::detail::NUM_THREADS_v<mmt>);
    std::cout << "Launching warmup kernel with grid (" << grid.x << ", " << grid.y << "), block (" << block.x << ")\n";
    for(int i = 0; i < ( 2 ); i++) { // warmup
        inner_run<mmt>(d_A, d_B, d_C, d_scale_a, d_scale_b, M, N, K, grid, block); 
    }

    // Start timing
    dpct::get_current_device().queues_wait_and_throw();
    std::cout << "Launching kernel with grid (" << grid.x << ", " << grid.y << "), block (" << block.x << ")\n";
    auto start = std::chrono::high_resolution_clock::now();

    constexpr int ITERS = ( 10 );
    for(int i = 0; i < ITERS; i++) {
        inner_run<mmt>(d_A, d_B, d_C, d_scale_a, d_scale_b, M, N, K, grid, block); 
    }
    dpct::get_current_device().queues_wait_and_throw();

    // End timing
    auto end = std::chrono::high_resolution_clock::now();

    // Calculate duration
    std::chrono::duration<double> diff = end - start;
    double useconds = diff.count() * 1e6 / ITERS;

    // Calculate TFLOPs
    double flops = double(2.0) * M * N * K; // 2 FLOPs per multiply-add
    double tflops = (flops / useconds) / 1e6;

    std::cout << "Avg Kernel execution time: " << useconds << " us\n";
    std::cout << "Achieved performance: " << tflops << " TFLOPs\n";
    
    // Check for CUDA errors
    /*
    DPCT1010:594: SYCL uses exceptions to report errors and does not use the
    error codes. The cudaGetLastError function call was replaced with 0. You
    need to rewrite this code.
    */
    cudaStatus = 0;
    /*
    DPCT1000:591: Error handling if-stmt was detected but could not be
    rewritten.
    */
    if (cudaStatus != 0) {
        /*
        DPCT1009:595: SYCL reports errors using exceptions and does not use
        error codes. Please replace the "get_error_string_dummy(...)" with a
        real error-handling function.
        */
        /*
        DPCT1001:590: The statement could not be removed.
        */
        std::cerr << "CUDA error: " << dpct::get_error_string_dummy(cudaStatus)
                  << std::endl;
        // Optionally, you might want to exit the program or handle the error in some way
        return -1;
    }

    // Copy result back to host
    c_dtype *h_C_out = new c_dtype[M * N];
    dpct::get_in_order_queue()
        .memcpy(h_C_out, d_C, M * N * sizeof(c_dtype))
        .wait();

    std::cout << "Copied result back to host" << std::endl;

    // Convert result back to float for comparison
    for (int i = 0; i < M * N; ++i) {
        h_C[i] = float(h_C_out[i]);
    }

    std::cout << "Converted result back to float" << std::endl;

    // Check result
    float max_error = 0.0f, total_error = 0.0f, total_ref = 0.0f, total_ours=0.0f;
    float input_a = 0.0f, input_b = 0.0f;
    int error_count = 0;
    printf("Num rows: %zu, Num cols: %zu\n", M, N);
    for (int i = 0; i < M * N; ++i) {
        float error = std::abs(h_C[i] - h_C_ref[i]);
        if( error > 0.10 ) { // large because of fp8 vs fp32 numerics # error > 0.10
            if(error_count < 10) std::cout << "Error at row " << i / N << " col " << i % N << ": " << h_C[i] << " != " << h_C_ref[i] << " (ref)" << std::endl;
            else if(error_count == 700) std::cout << "Too many errors to show them all.\n";
            error_count++;
        }
        total_ref += std::abs(h_C_ref[i]);
        total_error += error;
        total_ours += std::abs(h_C[i]);
    }

    for (int i = 0; i < M * K; i++) {
        input_a += std::abs(h_A[i]);
    }
    for (int i = 0; i < K * N; i++) {
        input_b += std::abs(h_B[i]);
    }

    std::cout << std::fixed << std::setprecision(6);
    std::cout << "Max error: " << max_error << std::endl;
    std::cout << "Average error: " << total_error / M / N << std::endl;
    std::cout << "Average ref: " << total_ref / (M * N) << std::endl;
    std::cout << "Average ours: " << total_ours / M / N << std::endl;
    std::cout << "Average input_a: " << input_a / M / K << std::endl;
    std::cout << "Average input_b: " << input_b / K / N << std::endl;
    std::cout << "Error count: " << error_count << std::endl;
 
    // write_matrix_to_csv("h_C_ref.csv", h_C_ref, M, N);
    // write_matrix_to_csv("h_C.csv", h_C, M, N);

    // Clean up
    delete[] h_A;
    delete[] h_B;
    delete[] h_C;
    delete[] h_C_ref;
    delete[] h_C_out;
    dpct::dpct_free(d_A, dpct::get_in_order_queue());
    dpct::dpct_free(d_B, dpct::get_in_order_queue());
    dpct::dpct_free(d_C, dpct::get_in_order_queue());

    return 0;
}


int main() {
    int M = 4096, N = 4096, K = 4096;
    run_benchmark<matmul_template<8>>(M, N, K);
    return 0;
}