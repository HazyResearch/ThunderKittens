#define DEFAULT_WGMMA_N 4 // must be 4 due to WGMMA
#define WORKERS 4

template<int WGMMA_K, int WGMMA_M>
__global__ void test_wgmma_tma_mma_ker_trans(CUtensorMap* tma_desc_a, CUtensorMap* tma_desc_b, CUtensorMap* tma_desc_out) {

    int warpid   = kittens::warpid();

    extern __shared__ alignment_dummy __shm22[]; // this is the CUDA shared memory
    shared_allocator al = shared_allocator::create_allocator_tma((int*)&__shm22[0]); 

    rt_fl<1, WGMMA_M> result;

    rt_bf<WGMMA_K, WGMMA_M> tmp_loader;
    
    using L1 = ducks::st_layout::wgmma_row_0b;
    using L2 = ducks::st_layout::wgmma_col_t_0b;
    st_bf<DEFAULT_WGMMA_N, WGMMA_K, L1> &a_smem = al.allocate<st_bf<DEFAULT_WGMMA_N, WGMMA_K, L1>>();
    st_bf<WGMMA_K, WGMMA_M, L2> &b_smem = al.allocate<st_bf<WGMMA_K, WGMMA_M, L2>>();
    st_bf<DEFAULT_WGMMA_N, WGMMA_M, ducks::st_layout::naive> &result_smem = al.allocate<st_bf<DEFAULT_WGMMA_N, WGMMA_M, ducks::st_layout::naive>>();

    auto block = cooperative_groups::this_thread_block();
    __shared__ uint64_t smem_barrier;  
    constexpr int size_bytes_a = sizeof(bf16) * a_smem.num_elements;
    constexpr int size_bytes_b = sizeof(bf16) * b_smem.num_elements;
    constexpr int size_bytes_result = sizeof(bf16) * result_smem.num_elements;

    if (warpid == 0) {
        tma::init_barrier(smem_barrier, 32);
        tma::set_barrier_bytes(smem_barrier, size_bytes_a+size_bytes_b);
        __syncwarp();

        tma::load_async(a_smem, tma_desc_a, 0, smem_barrier);
        tma::load_async(b_smem, tma_desc_b, 0, smem_barrier);

        constexpr int kPhaseBit = 1;
        tma::arrive_wait(smem_barrier, kPhaseBit);
    }

    __syncthreads();
    warpgroup::fence(result);
    warpgroup::mma_reset(result, a_smem, b_smem);
    warpgroup::commit_group();
    warpgroup::mma_async_wait();
    
    warpgroup::store(result_smem, result); // this is going to break
    if (warpid == 0) {
        tma::set_barrier_bytes(smem_barrier, size_bytes_result);
        tma::store_async(tma_desc_out, result_smem, 0);
        tma::commit_group();
        tma::wait_for_store_complete<0>();
    }
    __syncthreads();
}
template<int WGMMA_K, int WGMMA_M>
bool test_wgmma_tma_mma_trans() {
    // initailize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(256*WGMMA_K*(DEFAULT_WGMMA_N+WGMMA_M));
    std::vector<float> o_ref(256*DEFAULT_WGMMA_N*WGMMA_M);
    initialize(&d_i, &d_o, i_ref, o_ref);

    CUtensorMap tma_desc_input_a = {}, tma_desc_input_b = {}, tma_desc_output = {};
    tma::create_tensor_map<st_bf<DEFAULT_WGMMA_N, WGMMA_K, ducks::st_layout::wgmma_row_0b>, 1>(&tma_desc_input_a, d_i); 
    tma::create_tensor_map<st_bf<WGMMA_K, WGMMA_M, ducks::st_layout::wgmma_col_t_0b>, 1>(&tma_desc_input_b, d_i+256*DEFAULT_WGMMA_N*WGMMA_K); 
    tma::create_tensor_map<st_bf<DEFAULT_WGMMA_N, WGMMA_M, ducks::st_layout::naive>, 1>(&tma_desc_output, d_o); 

    CUtensorMap* tma_desc_input_a_d, * tma_desc_input_b_d, * tma_desc_output_d;
    cudaMalloc(&tma_desc_input_a_d, sizeof(CUtensorMap));
    cudaMalloc(&tma_desc_input_b_d, sizeof(CUtensorMap));
    cudaMalloc(&tma_desc_output_d, sizeof(CUtensorMap));
    cudaMemcpy(tma_desc_input_b_d, &tma_desc_input_b, sizeof(CUtensorMap), cudaMemcpyHostToDevice);
    cudaMemcpy(tma_desc_input_a_d, &tma_desc_input_a, sizeof(CUtensorMap), cudaMemcpyHostToDevice);
    cudaMemcpy(tma_desc_output_d, &tma_desc_output, sizeof(CUtensorMap), cudaMemcpyHostToDevice);

    // run kernel
    cudaFuncSetAttribute(test_wgmma_tma_mma_ker_trans<WGMMA_K, WGMMA_M>, cudaFuncAttributeMaxDynamicSharedMemorySize, 200000);
    test_wgmma_tma_mma_ker_trans<WGMMA_K, WGMMA_M><<<1, 128, 200000>>>(tma_desc_input_a_d, tma_desc_input_b_d, tma_desc_output_d);
    // fill in correct results on cpu
    for(int i = 0; i < DEFAULT_WGMMA_N*16; i++) {
        for(int j = 0; j < WGMMA_M*16; j++) {
            float sum = 0;
            for(int k = 0; k < WGMMA_K*16; k++) {
                sum += i_ref[i*WGMMA_K*16+k]*i_ref[256*WGMMA_K*DEFAULT_WGMMA_N + k*16*WGMMA_M + j];
            }
            o_ref[i*WGMMA_M*16+j] = sum;
        }
    }
    // check and cleanup
    std::string reg_label = "st_st_";
    bool passed = validate(
        d_i, d_o, i_ref, o_ref, 
        "wgmma_tma_mma_"+reg_label+std::to_string(DEFAULT_WGMMA_N)+"x"+std::to_string(WGMMA_K)+"x"+std::to_string(WGMMA_M),
        WGMMA_M*16
    );
    return passed;
}


int wgmma_tma_tests() {
    std::cout << " ----- Starting Integrated TMA+WGMMA tests! -----" << std::endl;
    int failures = 0;

    // test all combinations of wgmma sizes
    failures += !test_wgmma_tma_mma_trans<1, 1>();
    failures += !test_wgmma_tma_mma_trans<1, 2>();
    failures += !test_wgmma_tma_mma_trans<1, 3>();
    failures += !test_wgmma_tma_mma_trans<1, 4>();
    failures += !test_wgmma_tma_mma_trans<2, 1>();
    failures += !test_wgmma_tma_mma_trans<2, 2>();
    failures += !test_wgmma_tma_mma_trans<2, 3>();
    failures += !test_wgmma_tma_mma_trans<2, 4>();
    failures += !test_wgmma_tma_mma_trans<3, 1>();
    failures += !test_wgmma_tma_mma_trans<3, 2>();
    failures += !test_wgmma_tma_mma_trans<3, 3>();
    failures += !test_wgmma_tma_mma_trans<3, 4>();
    failures += !test_wgmma_tma_mma_trans<4, 1>();
    failures += !test_wgmma_tma_mma_trans<4, 2>(); // numerical instability
    failures += !test_wgmma_tma_mma_trans<4, 3>();
    failures += !test_wgmma_tma_mma_trans<4, 4>();
    
    return failures;
}