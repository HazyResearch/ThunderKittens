

template<st_wgmma_row_layout L, int WGMMA_NDIV4, int WGMMA_K, int WGMMA_M>
__global__ void test_wgmma_tall_dot_ker(const bf16 *input, bf16 *output) {
    constexpr int WGMMA_N = WGMMA_NDIV4*4;

    int warpid = kittens::warp_id(); 

    extern __shared__ alignment_dummy __shm22[]; // this is the CUDA shared memory
    shared_allocator al = shared_allocator::create_allocator_tma((int*)&__shm22[0]); 

    rt_fl<WGMMA_NDIV4, WGMMA_M> result;
    rt_bf<WGMMA_NDIV4, WGMMA_K> a_reg;
    
    st_bf<WGMMA_M, WGMMA_K, L> &b_smem = al.allocate<st_bf<WGMMA_M, WGMMA_K, L>>();

    // B always runs in shared memory
    warpgroup::load(b_smem, input+256*WGMMA_N*WGMMA_K, 16*WGMMA_K);

    // How to load A?
    warpgroup::load(a_reg, input, 16*WGMMA_K);
    __syncthreads();
    warpgroup::fence(result);

    warpgroup::dot_reset(result, a_reg, b_smem);
    warpgroup::commit_group();

    warpgroup::mma_async_wait();
    
    warpgroup::store(output, result, 16*WGMMA_M); // this is going to break
    __syncthreads();
}
template<st_wgmma_row_layout L, int WGMMA_NDIV4, int WGMMA_K, int WGMMA_M>
bool test_wgmma_tall_dot() {
    constexpr int WGMMA_N = WGMMA_NDIV4*4;
    // initialize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(256*WGMMA_K*(WGMMA_N+WGMMA_M));
    std::vector<float> o_ref(256*WGMMA_N*WGMMA_M);
    // for(int i = 0; i < 256*WGMMA_K*WGMMA_N; i++) i_ref[i] = float(i);
    // for(int i = 0; i < 16*WGMMA_K; i++) for(int j = 0; j < 16*WGMMA_M; j++) i_ref[256*WGMMA_K*WGMMA_N + i*16*WGMMA_M + j] = float(i==j);
    initialize<true>(&d_i, &d_o, i_ref, o_ref);
    // run kernel
    cudaFuncSetAttribute(test_wgmma_tall_dot_ker<L, WGMMA_NDIV4, WGMMA_K, WGMMA_M>, cudaFuncAttributeMaxDynamicSharedMemorySize, 100000);
    test_wgmma_tall_dot_ker<L, WGMMA_NDIV4, WGMMA_K, WGMMA_M><<<1, 128, 100000>>>(d_i, d_o); // need to launch a full warpgroup
    // fill in correct results on cpu
    for(int i = 0; i < WGMMA_N*16; i++) {
        for(int j = 0; j < WGMMA_M*16; j++) {
            float sum = 0;
            for(int k = 0; k < WGMMA_K*16; k++) {
                sum += i_ref[i*WGMMA_K*16+k]*i_ref[256*WGMMA_K*WGMMA_N + j*WGMMA_K*16+k];
            }
            o_ref[i*WGMMA_M*16+j] = sum;
        }
    }
    // check and cleanup
    std::string reg_label = "rt_st_";
    std::string layout_label = layout_name<L>()+"_";
    bool passed = validate(
        d_i, d_o, i_ref, o_ref, 
        "wgmma_tall_dot_"+reg_label+layout_label+std::to_string(WGMMA_N)+"x"+std::to_string(WGMMA_K)+"x"+std::to_string(WGMMA_M),
        WGMMA_M*16
    );
    return passed;
}
template<st_wgmma_col_layout L, int WGMMA_NDIV4, int WGMMA_K, int WGMMA_M>
__global__ void test_wgmma_tall_mma_ker(const bf16 *input, bf16 *output) {
    constexpr int WGMMA_N = WGMMA_NDIV4*4;

    int warpid = kittens::warp_id(); 

    extern __shared__ alignment_dummy __shm22[]; // this is the CUDA shared memory
    shared_allocator al = shared_allocator::create_allocator_tma((int*)&__shm22[0]); 

    rt_fl<WGMMA_NDIV4, WGMMA_M> result;
    rt_bf<WGMMA_NDIV4, WGMMA_K> a_reg;
    
    st_bf<WGMMA_K, WGMMA_M, L> &b_smem = al.allocate<st_bf<WGMMA_K, WGMMA_M, L>>();

    // B always runs in shared memory
    warpgroup::load(b_smem, input+256*WGMMA_N*WGMMA_K, 16*WGMMA_M);

    // How to load A?
    warpgroup::load(a_reg, input, 16*WGMMA_K);
    __syncthreads();
    warpgroup::fence(result);

    warpgroup::mma_reset(result, a_reg, b_smem);
    warpgroup::commit_group();

    warpgroup::mma_async_wait();
    
    warpgroup::store(output, result, 16*WGMMA_M); // this is going to break
    __syncthreads();
}
template<st_wgmma_col_layout L, int WGMMA_NDIV4, int WGMMA_K, int WGMMA_M>
bool test_wgmma_tall_mma() {
    constexpr int WGMMA_N = WGMMA_NDIV4*4;
    // initialize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(256*WGMMA_K*(WGMMA_N+WGMMA_M));
    std::vector<float> o_ref(256*WGMMA_N*WGMMA_M);
    // for(int i = 0; i < 256*WGMMA_K*WGMMA_N; i++) i_ref[i] = float(i);
    // for(int i = 0; i < 16*WGMMA_K; i++) for(int j = 0; j < 16*WGMMA_M; j++) i_ref[256*WGMMA_K*WGMMA_N + i*16*WGMMA_M + j] = float(i==j);
    initialize<true>(&d_i, &d_o, i_ref, o_ref);
    // run kernel
    cudaFuncSetAttribute(test_wgmma_tall_mma_ker<L, WGMMA_NDIV4, WGMMA_K, WGMMA_M>, cudaFuncAttributeMaxDynamicSharedMemorySize, 100000);
    test_wgmma_tall_mma_ker<L, WGMMA_NDIV4, WGMMA_K, WGMMA_M><<<1, 128, 100000>>>(d_i, d_o); // need to launch a full warpgroup
    // fill in correct results on cpu
    for(int i = 0; i < WGMMA_N*16; i++) {
        for(int j = 0; j < WGMMA_M*16; j++) {
            float sum = 0;
            for(int k = 0; k < WGMMA_K*16; k++) {
                sum += i_ref[i*WGMMA_K*16+k]*i_ref[256*WGMMA_K*WGMMA_N + k*WGMMA_M*16+j];
            }
            o_ref[i*WGMMA_M*16+j] = sum;
        }
    }
    // check and cleanup
    std::string reg_label = "rt_st_";
    std::string layout_label = layout_name<L>()+"_";
    bool passed = validate(
        d_i, d_o, i_ref, o_ref, 
        "wgmma_tall_mma_"+reg_label+layout_label+std::to_string(WGMMA_N)+"x"+std::to_string(WGMMA_K)+"x"+std::to_string(WGMMA_M),
        WGMMA_M*16
    );
    return passed;
}


template<st_wgmma_row_layout L>
int test_tall_wgmma_dot_layout() {
    int failures = 0;

    failures += !test_wgmma_tall_dot<L, 2, 4, 1>();
    failures += !test_wgmma_tall_dot<L, 2, 4, 2>();
    failures += !test_wgmma_tall_dot<L, 2, 4, 3>();
    failures += !test_wgmma_tall_dot<L, 2, 4, 4>();
    failures += !test_wgmma_tall_dot<L, 3, 4, 1>();
    failures += !test_wgmma_tall_dot<L, 3, 4, 2>();
    failures += !test_wgmma_tall_dot<L, 3, 4, 3>();
    failures += !test_wgmma_tall_dot<L, 3, 4, 4>();
    failures += !test_wgmma_tall_dot<L, 4, 4, 1>();
    failures += !test_wgmma_tall_dot<L, 4, 4, 2>();
    failures += !test_wgmma_tall_dot<L, 4, 4, 3>();
    failures += !test_wgmma_tall_dot<L, 4, 4, 4>();

    return failures;
}

template<st_wgmma_col_layout L>
int test_tall_wgmma_mma_layout() {
    int failures = 0;

    failures += !test_wgmma_tall_mma<L, 2, 4, 1>();
    failures += !test_wgmma_tall_mma<L, 2, 4, 2>();
    failures += !test_wgmma_tall_mma<L, 2, 4, 3>();
    failures += !test_wgmma_tall_mma<L, 2, 4, 4>();
    failures += !test_wgmma_tall_mma<L, 3, 4, 1>();
    failures += !test_wgmma_tall_mma<L, 3, 4, 2>();
    failures += !test_wgmma_tall_mma<L, 3, 4, 3>();
    failures += !test_wgmma_tall_mma<L, 3, 4, 4>();
    failures += !test_wgmma_tall_mma<L, 4, 4, 1>();
    failures += !test_wgmma_tall_mma<L, 4, 4, 2>();
    failures += !test_wgmma_tall_mma<L, 4, 4, 3>();
    failures += !test_wgmma_tall_mma<L, 4, 4, 4>();

    return failures;
}


int tall_wgmma_tests() {
    std::cout << " ----- Starting Tall Warpgroup MMA tests! -----" << std::endl;
    int failures = 0;

    failures += test_tall_wgmma_dot_layout<st_wgmma_row_0b_layout>();
    failures += test_tall_wgmma_dot_layout<st_wgmma_row_32b_layout>();

    failures += test_tall_wgmma_mma_layout<st_wgmma_col_t_0b_layout>();

    return failures;
}