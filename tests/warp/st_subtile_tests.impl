#define SUBTILE_N 4
#define SUBTILE_ROWS (SUBTILE_N*16)
#define SUBTILE_COLS (SUBTILE_N*16)
#define SUBTILE_SIZE (SUBTILE_ROWS*SUBTILE_COLS)

template<st_layout layout>
__global__ void test_st_subtile_row_ker(const bf16 *input, bf16 *output) {

    int warpid = threadIdx.x / 32;

    extern __shared__ alignment_dummy __shm_subtile_row[]; // this is the CUDA shared memory
    shared_allocator alloc = shared_allocator::create_allocator((int*)&__shm_subtile_row[0]);

    rt_fl<1, SUBTILE_N> reg_tile;
    st_bf<SUBTILE_N, SUBTILE_N, layout> &smem_tile = alloc.allocate<st_bf<SUBTILE_N, SUBTILE_N, layout>>();

    if(warpid == 0) load(smem_tile, input, SUBTILE_COLS);
    __syncthreads();

    auto subtile = smem_tile.template subtile<1,SUBTILE_N>(warpid, 0);

    load(reg_tile, subtile);

    add(reg_tile, reg_tile, float(warpid)); // prove each warp is doing its own work

    store(subtile, reg_tile);

    __syncthreads(); // everyone done?

    if(warpid == 0) store(output, smem_tile, SUBTILE_COLS); // great, write to global
}
template<st_layout layout>
bool test_st_subtile_row() {
    // initailize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(SUBTILE_SIZE);
    std::vector<float> o_ref(SUBTILE_SIZE);
    initialize(&d_i, &d_o, i_ref, o_ref);
    // run kernel
    cudaFuncSetAttribute(test_st_subtile_row_ker<layout>, cudaFuncAttributeMaxDynamicSharedMemorySize, 100000);
    test_st_subtile_row_ker<layout><<<1, 128, 100000>>>(d_i, d_o); // use a warpgroup to do this to make sure it really works
    // fill in correct results on cpu
    for(int i = 0; i < SUBTILE_ROWS; i++) {
        for(int j = 0; j < SUBTILE_COLS; j++) {
            o_ref[i*SUBTILE_COLS + j] = i_ref[i*SUBTILE_COLS + j] + float(i / 16);
        }
    }
    // check and cleanup
    bool passed = validate(d_i, d_o, i_ref, o_ref, "test_subtile_row_"+layout_name<layout>(), SUBTILE_COLS);
    return passed;
}

template<st_layout layout>
__global__ void test_st_subtile_col_ker(const bf16 *input, bf16 *output) {

    int warpid = threadIdx.x / 32;

    extern __shared__ alignment_dummy __shm_subtile_row[]; // this is the CUDA shared memory
    shared_allocator alloc = shared_allocator::create_allocator((int*)&__shm_subtile_row[0]);

    rt_fl<SUBTILE_N, 1> reg_tile;
    st_bf<SUBTILE_N, SUBTILE_N, layout> &smem_tile = alloc.allocate<st_bf<SUBTILE_N, SUBTILE_N, layout>>();

    if(warpid == 0) load(smem_tile, input, SUBTILE_COLS);
    __syncthreads();

    auto subtile = smem_tile.template subtile<SUBTILE_N,1>(0,warpid);

    load(reg_tile, subtile);

    add(reg_tile, reg_tile, float(warpid)); // prove each warp is doing its own work

    store(subtile, reg_tile);

    __syncthreads(); // everyone done?

    if(warpid == 0) store(output, smem_tile, SUBTILE_COLS); // great, write to global
}
template<st_layout layout>
bool test_st_subtile_col() {
    // initailize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(SUBTILE_SIZE);
    std::vector<float> o_ref(SUBTILE_SIZE);
    initialize(&d_i, &d_o, i_ref, o_ref);
    // run kernel
    cudaFuncSetAttribute(test_st_subtile_col_ker<layout>, cudaFuncAttributeMaxDynamicSharedMemorySize, 100000);
    test_st_subtile_col_ker<layout><<<1, 128, 100000>>>(d_i, d_o); // use a warpgroup to do this to make sure it really works
    // fill in correct results on cpu
    for(int i = 0; i < SUBTILE_ROWS; i++) {
        for(int j = 0; j < SUBTILE_COLS; j++) {
            o_ref[i*SUBTILE_COLS + j] = i_ref[i*SUBTILE_COLS + j] + float(j / 16);
        }
    }
    // check and cleanup
    bool passed = validate(d_i, d_o, i_ref, o_ref, "test_subtile_col_"+layout_name<layout>(), SUBTILE_COLS);
    return passed;
}

int st_subtile_tests() {
    std::cout << " ----- Starting shared subtile tests! -----" << std::endl;
    int failures = 0;
    failures += !test_st_subtile_row<st_naive_row_0b_layout      >();
    failures += !test_st_subtile_row<st_xor_row_layout        >();
    failures += !test_st_subtile_row<st_wgmma_row_0b_layout   >();
    failures += !test_st_subtile_row<st_wgmma_row_32b_layout  >();
    failures += !test_st_subtile_row<st_wgmma_col_t_0b_layout >();
    failures += !test_st_subtile_row<st_wgmma_col_t_32b_layout>();

    failures += !test_st_subtile_col<st_naive_row_0b_layout      >();
    failures += !test_st_subtile_col<st_xor_row_layout        >();
    failures += !test_st_subtile_col<st_wgmma_row_0b_layout   >();
    failures += !test_st_subtile_col<st_wgmma_row_32b_layout  >();
    failures += !test_st_subtile_col<st_wgmma_col_t_0b_layout >();
    failures += !test_st_subtile_col<st_wgmma_col_t_32b_layout>();
    return failures;
}