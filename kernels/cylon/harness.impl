#include <iostream>
#include <fstream>
#include <sstream>
#include <vector>
#include <cmath>
#include <cuda_runtime.h>
#include <cuda_bf16.h>

constexpr int LOAD_B            = 1;
constexpr int B                 = 64;
constexpr int H                 = 1;
constexpr int N                 = 8192;
constexpr int D_QK              = 64;
constexpr int D_VO              = 128;
// constexpr int COLLABORATIVE_SMS = 8;

// Error checking macro
#define CudaCheckError() { \
    cudaError_t e=cudaGetLastError(); \
    if(e!=cudaSuccess) { \
        printf("Cuda failure %s:%d: '%s'\n",__FILE__,__LINE__,cudaGetErrorString(e)); \
        exit(0); \
    } \
}

uint64_t flops(int b, int n, int d_qk, int d_vo, int collab_size) {
    uint64_t flops_per_sm = (64*64*64*2 * 8 * 2); // 8 MMA's, two consumer warpgroups
    flops_per_sm *= n / 64;
    return flops_per_sm * b * collab_size;
}
uint64_t flops_bwd(int b, int n, int d_qk, int d_vo, int collab_size) {
    uint64_t flops_per_sm = (64*64*64*2 * 8 * 2); // 8 MMA's, two consumer warpgroups
    flops_per_sm *= n / 64;
    return flops_per_sm * b * collab_size * 2; // we need to launch two blocks per forward pass block
}

std::vector<float> loadDataFromFile(const std::string& filename, int expectedSize) {
    std::vector<float> data;
    std::ifstream file(filename);
    
    if (!file.is_open()) {
        std::cerr << "Error opening file: " << filename << std::endl;
        return data;
    }

    float value;
    while (file >> value) {
        data.push_back(value);
    }

    if (data.size() != expectedSize) {
        std::cerr << "Warning: File " << filename << " contains " << data.size() 
                  << " elements, expected " << expectedSize << std::endl;
    }

    return data;
}

void convertToBF16(const std::vector<float>& input, std::vector<bf16>& output) {
    for (int i = 0; i < output.size(); ++i) {
        output[i] = __float2bfloat16(input[i%input.size()]);
    }
}

template<typename T>
void compareOutputs(const std::vector<T>& computed, const std::vector<float>& reference, int size, float print_tolerance) {
    int mismatches[4] = {0, 0, 0, 0};
    float total_error = 0.0f;
    float total_magnitude = 0.0f;

    for (int i = 0; i < size; ++i) {
        float comp;
        if constexpr (std::is_same_v<T, bf16>) {
            comp = __bfloat162float(computed[i]);
        } else if constexpr (std::is_same_v<T, float>) {
            comp = computed[i];
        }
        float ref = reference[i%reference.size()];
        float diff = std::abs(comp - ref);
        
        total_error += diff;
        total_magnitude += std::abs(ref);

        if (diff > 0.0001) mismatches[0]++;
        if (diff > 0.001) mismatches[1]++;
        if (diff > 0.01) mismatches[2]++;
        if (diff > 0.1) mismatches[3]++;

        if (diff > print_tolerance && mismatches[0] <= 10) {
            std::cout << "Mismatch at index " << i << ": Computed = " << comp << ", Reference = " << ref << std::endl;
        }
    }
    
    float avg_error = total_error / size;
    float avg_magnitude = total_magnitude / size;
    float avg_percent_diff = (avg_error / avg_magnitude) * 100;

    std::cout << "Mismatches > 0.0001: " << mismatches[0] << " (" << (float)mismatches[0] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.001: " << mismatches[1] << " (" << (float)mismatches[1] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.01: " << mismatches[2] << " (" << (float)mismatches[2] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.1: " << mismatches[3] << " (" << (float)mismatches[3] / size * 100 << "%)" << std::endl;
    std::cout << "Average error: " << avg_error << std::endl;
    std::cout << "Average magnitude of reference: " << avg_magnitude << std::endl;
    std::cout << "Average percent difference: " << avg_percent_diff << "%" << std::endl;
}

template<typename T> void dumpToFile(const std::string& filename, const std::vector<T>& data, size_t size, size_t lineWidth) {
    try {
        std::ofstream file(filename);
        if (!file.is_open()) {
            throw std::runtime_error("Unable to open " + filename);
        }
        for (size_t i = 0; i < size; i++) {
            if constexpr (std::is_same_v<T, bf16>) {
                file << __bfloat162float(data[i]) << " ";
            } else if constexpr (std::is_same_v<T, float>) {
                file << data[i] << " ";
            }
            if ((i + 1) % lineWidth == 0) {
                file << "\n";
            }
        }
        file.close();
        std::cout << "Data dumped to " << filename << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "Error in dumpToFile: " << e.what() << std::endl;
        throw; // Re-throw the exception for higher-level handling
    }
}


int main() {
    // Load data from files
    auto q_data = loadDataFromFile("inputs/q.txt", LOAD_B * H * N * D_QK);
    auto k_data = loadDataFromFile("inputs/k.txt", LOAD_B * H * N * D_QK);
    auto v_data = loadDataFromFile("inputs/v.txt", LOAD_B * H * N * D_VO);
    auto q_map_data = loadDataFromFile("inputs/q_map.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    auto k_map_data = loadDataFromFile("inputs/k_map.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    auto o_grad_data = loadDataFromFile("inputs/o_grad.txt", LOAD_B * H * N * D_VO);
    auto ref_output_data = loadDataFromFile("reference/output.txt", LOAD_B * H * N * D_VO);
    auto ref_kv_state_data = loadDataFromFile("reference/kv_state.txt", LOAD_B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO);
    auto ref_q_grad_data = loadDataFromFile("reference/q_grad.txt", LOAD_B * H * N * D_QK);
    auto ref_k_grad_data = loadDataFromFile("reference/k_grad.txt", LOAD_B * H * N * D_QK);
    auto ref_v_grad_data = loadDataFromFile("reference/v_grad.txt", LOAD_B * H * N * D_VO);
    auto ref_q_map_grad_data = loadDataFromFile("reference/q_map_grad.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    auto ref_k_map_grad_data = loadDataFromFile("reference/k_map_grad.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    std::cout << "Loaded data from files" << std::endl;

    // Create arrays and convert data to bf16
    std::vector<bf16> q(B * H * N * D_QK);
    std::vector<bf16> k(B * H * N * D_QK);
    std::vector<bf16> v(B * H * N * D_VO);
    std::vector<float> kv_state(B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO);
    std::vector<bf16> q_map(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    std::vector<bf16> k_map(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    std::vector<bf16> o_grad(B * H * N * D_VO);

    std::vector<bf16> o(B * H * N * D_VO);
    std::vector<float> q_grad(B * H * N * D_QK);
    std::vector<float> k_grad(B * H * N * D_QK);
    std::vector<float> v_grad(B * H * N * D_VO);
    std::vector<float> q_map_grad(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    std::vector<float> k_map_grad(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);

    convertToBF16(q_data, q);
    convertToBF16(k_data, k);
    convertToBF16(v_data, v);
    convertToBF16(q_map_data, q_map);
    convertToBF16(k_map_data, k_map);
    convertToBF16(o_grad_data, o_grad);
    std::cout << "Converted data to bf16" << std::endl;

    // Create CUDA tensors
    bf16 *d_q, *d_k, *d_v, *d_o, *d_q_map, *d_k_map, *d_o_grad;
    float *d_kv_state, *d_q_grad, *d_k_grad, *d_v_grad, *d_q_map_grad, *d_k_map_grad;
    cudaMalloc(&d_q, B * H * N * D_QK * sizeof(bf16));
    cudaMalloc(&d_k, B * H * N * D_QK * sizeof(bf16));
    cudaMalloc(&d_v, B * H * N * D_VO * sizeof(bf16));
    cudaMalloc(&d_kv_state, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO * sizeof(float));
    cudaMalloc(&d_q_map, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16));
    cudaMalloc(&d_k_map, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16));
    cudaMalloc(&d_o_grad, B * H * N * D_VO * sizeof(bf16));
    cudaMalloc(&d_o, B * H * N * D_VO * sizeof(bf16));
    cudaMalloc(&d_q_grad, B * H * N * D_QK * sizeof(float));
    cudaMalloc(&d_k_grad, B * H * N * D_QK * sizeof(float));
    cudaMalloc(&d_v_grad, B * H * N * D_VO * sizeof(float));
    cudaMalloc(&d_q_map_grad, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(float));
    cudaMalloc(&d_k_map_grad, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(float));
    std::cout << "Allocated CUDA tensors" << std::endl;

    // Copy data to GPU
    cudaMemcpy(d_q, q.data(), B * H * N * D_QK * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k, k.data(), B * H * N * D_QK * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v, v.data(), B * H * N * D_VO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_q_map, q_map.data(), H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k_map, k_map.data(), H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_o_grad, o_grad.data(), B * H * N * D_VO * sizeof(bf16), cudaMemcpyHostToDevice);
    std::cout << "Copied data to GPU" << std::endl;

    // Create tma descriptors
    using q_tile        = st_bf<64, 64>;
    using k_tile        = st_bf<64, 64>;
    using v_tile        = st_bf<64, 64>;
    using o_tile        = st_bf<64, 64>;
    using o_grad_tile   = st_bf<64, 64>;
    using kv_state_tile = st_fl<64, 64>;
    using q_map_tile    = st_bf<64, 64>;
    using k_map_tile    = st_bf<64, 64>;

    using q_grad_tile        = st_fl<64, 64>;
    using k_grad_tile        = st_fl<64, 64>;
    using v_grad_tile        = st_fl<64, 64>;
    using q_map_grad_tile    = st_fl<64, 64>;
    using k_map_grad_tile    = st_fl<64, 64>;

    using q_global        = gl<bf16,  -1, -1, -1, -1, q_tile>;
    using k_global        = gl<bf16,  -1, -1, -1, -1, k_tile>;
    using v_global        = gl<bf16,  -1, -1, -1, -1, v_tile>;
    using o_global        = gl<bf16,  -1, -1, -1, -1, o_tile>;
    using o_grad_global   = gl<bf16,  -1, -1, -1, -1, o_grad_tile>;
    using kv_state_global = gl<float, -1, -1, -1, -1, kv_state_tile>;
    using q_map_global    = gl<bf16, -1, -1, -1, -1,  q_map_tile>;
    using k_map_global    = gl<bf16, -1, -1, -1, -1,  k_map_tile>;

    using q_grad_global        = gl<float, -1, -1, -1, -1, q_grad_tile>;
    using k_grad_global        = gl<float, -1, -1, -1, -1, k_grad_tile>;
    using v_grad_global        = gl<float, -1, -1, -1, -1, v_grad_tile>;
    using q_map_grad_global    = gl<float, -1, -1, -1, -1, q_map_grad_tile>;
    using k_map_grad_global    = gl<float, -1, -1, -1, -1, k_map_grad_tile>;

    using globals_fwd = fwd_globals; 
    using globals_bwd = bwd_globals;

    q_global           qg_arg{d_q,      B, H, N, D_QK};
    k_global           kg_arg{d_k,      B, H, N, D_QK};
    v_global           vg_arg{d_v,      B, H, N, D_VO};
    o_global           og_arg{d_o,      B, H, N, D_VO};
    o_grad_global og_grad_arg{d_o_grad, B, H, N, D_VO};

    kv_state_global kv_state_arg{d_kv_state, B, H, STATE_PER_SM * COLLABORATIVE_SMS * D_QK, D_VO};
    q_map_global       q_map_arg{d_q_map,    H, COLLABORATIVE_SMS, D_QK * STATE_PER_SM, D_QK};
    k_map_global       k_map_arg{d_k_map,    H, COLLABORATIVE_SMS, D_QK * STATE_PER_SM, D_QK};

    q_grad_global        qg_grad_arg{d_q_grad,     B, H, N, D_QK};
    k_grad_global        kg_grad_arg{d_k_grad,     B, H, N, D_QK};
    v_grad_global        vg_grad_arg{d_v_grad,     B, H, N, D_VO};
    q_map_grad_global q_map_grad_arg{d_q_map_grad, H, COLLABORATIVE_SMS, D_QK * STATE_PER_SM, D_QK};
    k_map_grad_global k_map_grad_arg{d_k_map_grad, H, COLLABORATIVE_SMS, D_QK * STATE_PER_SM, D_QK};

    globals_fwd g_fwd{qg_arg, kg_arg, vg_arg, og_arg,    kv_state_arg, q_map_arg,   k_map_arg, N};
    globals_bwd g_bwd{qg_arg, kg_arg, vg_arg, q_map_arg, k_map_arg,    og_grad_arg, kv_state_arg, qg_grad_arg, kg_grad_arg, vg_grad_arg, q_map_grad_arg, k_map_grad_arg, N};

    std::cout << "Created TMA descriptors" << std::endl;

    // Launch kernel
    cudaFuncSetAttribute(
        cylon_forwards, 
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        225000);

    dim3 grid(COLLABORATIVE_SMS, H, B);
    std::cout << "Launching kernel with grid of " << B << "x" << H << "x" << COLLABORATIVE_SMS << " and blocks of " << NUM_THREADS << " threads" << std::endl;
    auto start = std::chrono::high_resolution_clock::now();

    cylon_forwards<<<grid, NUM_THREADS, 225000>>>(g_fwd);
    CudaCheckError();

    // Wait for kernel to finish
    cudaDeviceSynchronize();
    CudaCheckError();
    std::cout << "Forward kernel finished" << std::endl;

    // PRINT UTILIZATION FOR FORWARD KERNEL //

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    std::cout << "Kernel execution time: " << duration.count() << " microseconds" << std::endl;
    uint64_t flops_executed = flops(B, N, D_QK, D_VO, COLLABORATIVE_SMS);
    std::cout << "FLOPs executed: " << flops_executed/1e12 << "T\n";
    std::cout << "TFLOPs achieved: " << flops_executed / duration.count() / 1e6 << std::endl;

    // CHECK CORRECTNESS FOR FORWARD KERNEL //

    // Copy data from GPU
    cudaMemcpy(o.data(), d_o, B * H * N * D_VO * sizeof(bf16), cudaMemcpyDeviceToHost);
    cudaMemcpy(kv_state.data(), d_kv_state, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO * sizeof(float), cudaMemcpyDeviceToHost);

    // Compare outputs
    std::cout << "Comparing outputs..." << std::endl;
    compareOutputs(o, ref_output_data, B * H * N * D_VO, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/o.txt", o, N * D_VO, D_VO);
    std::cout << "Comparing kv_state..." << std::endl;
    compareOutputs(kv_state, ref_kv_state_data, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO, 1e-3f);
    dumpToFile("outputs/kv_state.txt", kv_state, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO, D_VO);
    
    // RUN BACKWARDS KERNEL //

    // Launch kernel
    cudaFuncSetAttribute(cylon_backwards, cudaFuncAttributeMaxDynamicSharedMemorySize, 225000);

    dim3 bwd_grid(COLLABORATIVE_SMS*2, H, B);
    std::cout << "Launching kernel with grid of " << B << "x" << H << "x" << COLLABORATIVE_SMS*2 << " and blocks of " << NUM_THREADS << " threads" << std::endl;
    start = std::chrono::high_resolution_clock::now();

    cylon_backwards<<<bwd_grid, NUM_THREADS, 225000>>>(g_bwd);
    CudaCheckError();

    // Wait for kernel to finish
    cudaDeviceSynchronize();
    CudaCheckError();
    std::cout << "Backwards kernel finished" << std::endl;

    /* PRINT UTILIZATION FOR BACKWARDS KERNEL */

    end = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    std::cout << "Kernel execution time: " << duration.count() << " microseconds" << std::endl;
    flops_executed = flops_bwd(B, N, D_QK, D_VO, COLLABORATIVE_SMS);
    std::cout << "FLOPs executed: " << flops_executed/1e12 << "T\n";
    std::cout << "TFLOPs achieved: " << flops_executed / duration.count() / 1e6 << std::endl;

    /* CHECK CORRECTNESS FOR BACKWARDS KERNEL */

    cudaMemcpy(q_grad.data(), d_q_grad, B * H * N * D_QK * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "Copied q grad to host" << std::endl;
    cudaMemcpy(k_grad.data(), d_k_grad, B * H * N * D_QK * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "Copied k grad to host" << std::endl;
    cudaMemcpy(v_grad.data(), d_v_grad, B * H * N * D_VO * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "Copied v grad to host" << std::endl;
    cudaMemcpy(q_map_grad.data(), d_q_map_grad, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "Copied q map grad to host" << std::endl;
    cudaMemcpy(k_map_grad.data(), d_k_map_grad, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "Copied k map grad to host" << std::endl;
    for(int i = 0; i < q_map_grad.size(); i++) { // these have to be normalized because the batch dim is kind of fake here
        q_map_grad[i] *= float(LOAD_B) / float(B);
        k_map_grad[i] *= float(LOAD_B) / float(B);
    }

    // Compare outputs
    std::cout << "Comparing q grad..." << std::endl;
    compareOutputs(q_grad, ref_q_grad_data, B * H * N * D_QK, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/q_grad.txt", q_grad, N * D_QK, D_QK);
    std::cout << "Comparing k grad..." << std::endl;
    compareOutputs(k_grad, ref_k_grad_data, B * H * N * D_QK, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/k_grad.txt", k_grad, N * D_QK, D_QK);
    std::cout << "Comparing v grad..." << std::endl;
    compareOutputs(v_grad, ref_v_grad_data, B * H * N * D_VO, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/v_grad.txt", v_grad, N * D_VO, D_VO);
    std::cout << "Comparing q map grad..." << std::endl;
    compareOutputs(q_map_grad, ref_q_map_grad_data, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/q_map_grad.txt", q_map_grad, D_QK * D_QK * STATE_PER_SM, D_QK * D_QK);
    std::cout << "Comparing k map grad..." << std::endl;
    compareOutputs(k_map_grad, ref_k_map_grad_data, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM, 1e-3f); // Adjust tolerance as needed
    dumpToFile("outputs/k_map_grad.txt", k_map_grad, D_QK * D_QK * STATE_PER_SM, D_QK * D_QK);

    // Free memory
    cudaFree(d_q);
    cudaFree(d_k);
    cudaFree(d_v);
    cudaFree(d_o);
    cudaFree(d_q_map);
    cudaFree(d_k_map);
    cudaFree(d_kv_state);
    cudaFree(d_q_grad);
    cudaFree(d_k_grad);
    cudaFree(d_v_grad);
    cudaFree(d_q_map_grad);
    cudaFree(d_k_map_grad);
    cudaFree(d_o_grad);

    std::cout << "Execution completed successfully!" << std::endl;
    return 0;
}